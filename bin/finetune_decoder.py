#!/usr/bin/env python3
"""
Phase 2: Text-only Fine-tuning of Whisper Decoder

This script fine-tunes the Whisper decoder using pseudo encoder states generated by a trained TLE model.
The goal is to adapt Whisper for text-only inference while maintaining multilingual capabilities.

Usage:
    python bin/finetune_decoder.py --tle_checkpoint checkpoints/tle-final.ckpt --dataset "mozilla-foundation/common_voice_11_0" --output_dir checkpoints/whisper-finetuned
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import lightning.pytorch as pl
from lightning.pytorch import Trainer
from lightning.pytorch.callbacks import ModelCheckpoint
from lightning.pytorch.loggers import WandbLogger
import argparse
import os
from transformers import WhisperProcessor, WhisperModel
from datasets import load_dataset

from whistle.tle.tle import TLEVAE, TLEVAEConfig


def load_whisper_model(
    device: str = "cuda",
) -> tuple[WhisperModel, WhisperProcessor]:
    """Load Whisper model components."""
    if not torch.cuda.is_available() and device == "cuda":
        device = "cpu"

    print("Loading Whisper model...")
    whisper_model = WhisperModel.from_pretrained("openai/whisper-large-v3")
    whisper_model = whisper_model.to(device)

    processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3")

    return whisper_model, processor


def load_tle_model(checkpoint_path: str, device: str = "cuda") -> TLEVAE:
    """Load trained TLE model from checkpoint."""
    print(f"Loading TLE model from {checkpoint_path}...")

    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # Reconstruct config from checkpoint
    cfg = TLEVAEConfig(**checkpoint["hyper_parameters"]["cfg"])

    # Create model and load state
    model = TLEVAE(cfg)
    model.load_state_dict(checkpoint["state_dict"])
    model = model.to(device)
    model.eval()

    return model


class WhisperDecoderFinetune(pl.LightningModule):
    """
    Fine-tunes Whisper decoder using TLE-generated pseudo encoder states.
    """

    def __init__(
        self,
        tle_model: TLEVAE,
        whisper_model: WhisperModel,
        processor: WhisperProcessor,
        learning_rate: float = 1e-5,
        label_smoothing: float = 0.1,
    ):
        super().__init__()
        self.tle_model = tle_model
        self.whisper = whisper_model
        self.processor = processor
        self.learning_rate = learning_rate
        self.label_smoothing = label_smoothing

        # Freeze TLE model
        for param in self.tle_model.parameters():
            param.requires_grad = False

        # Freeze Whisper encoder
        for param in self.whisper.encoder.parameters():
            param.requires_grad = False

        # Only train decoder parameters
        self.decoder_params = list(self.whisper.decoder.parameters())

    def forward(self, input_ids, attention_mask, lang_ids=None):
        """
        Generate pseudo encoder states from text using TLE.
        """
        batch_size, seq_len = input_ids.shape

        # Generate pseudo encoder states using TLE
        with torch.no_grad():
            # Estimate target length from text length (heuristic)
            target_T = max(50, seq_len * 3)  # Rough estimate: ~3 frames per token

            E_pseudo, _, _, _ = self.tle_model(
                input_ids, attention_mask, target_T=target_T, lang_ids=lang_ids
            )

        return E_pseudo

    def training_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]  # Raw text tokens for TLE
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]  # Full target sequence for decoder
        lang_ids = batch.get("lang_ids")

        # Get pseudo encoder states from TLE using raw text
        with torch.no_grad():
            # Estimate target length from text length
            target_T = max(50, input_ids.size(1) * 3)  # Rough heuristic

            E_pseudo, _, _ = self.tle_model(
                input_ids, attention_mask, target_T=target_T, lang_ids=lang_ids
            )

        # Decoder input: full target sequence shifted by 1 (teacher forcing)
        # Input gets all tokens except the last, labels get all tokens except the first
        decoder_input_ids = labels[:, :-1]  # (batch_size, seq_len-1)
        decoder_labels = labels[:, 1:]  # (batch_size, seq_len-1)

        # Forward pass through Whisper decoder
        decoder_outputs = self.whisper.decoder(
            input_ids=decoder_input_ids,
            encoder_outputs=E_pseudo,  # (batch_size, time, hidden)
        )

        # Compute cross-entropy loss
        logits = decoder_outputs.logits  # (batch_size, seq_len-1, vocab_size)
        loss_fct = nn.CrossEntropyLoss(
            label_smoothing=self.label_smoothing, ignore_index=-100
        )

        # Flatten for loss computation
        loss = loss_fct(
            logits.reshape(
                -1, logits.size(-1)
            ),  # (batch_size * (seq_len-1), vocab_size)
            decoder_labels.reshape(-1),  # (batch_size * (seq_len-1),)
        )

        self.log("train_loss", loss, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]
        lang_ids = batch.get("lang_ids")

        # Get pseudo encoder states
        with torch.no_grad():
            target_T = max(50, input_ids.size(1) * 3)
            E_pseudo, _, _ = self.tle_model(
                input_ids, attention_mask, target_T=target_T, lang_ids=lang_ids
            )

        # Decoder input: full target sequence shifted by 1 (teacher forcing)
        decoder_input_ids = labels[:, :-1]  # (batch_size, seq_len-1)
        decoder_labels = labels[:, 1:]  # (batch_size, seq_len-1)

        # Forward pass
        with torch.no_grad():
            decoder_outputs = self.whisper.decoder(
                input_ids=decoder_input_ids,
                encoder_outputs=E_pseudo,
            )

            logits = decoder_outputs.logits
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(
                logits.reshape(-1, logits.size(-1)), decoder_labels.reshape(-1)
            )

        self.log("val_loss", loss, prog_bar=True)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.decoder_params, lr=self.learning_rate, weight_decay=0.01
        )
        return optimizer


class TextOnlyDataset(torch.utils.data.Dataset):
    """
    Dataset for text-only fine-tuning.
    Generates target labels for decoder training.
    """

    def __init__(self, hf_dataset, processor, split="train", max_length=256):
        # Handle different dataset types: HF datasets vs mock datasets
        if split is not None and hasattr(hf_dataset, "keys") and split in hf_dataset:
            self.dataset = hf_dataset[split]
        else:
            self.dataset = hf_dataset
        self.processor = processor
        self.max_length = max_length

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]

        # Get text and language
        text = item["text"]
        lang = item.get("language", "en")

        # Tokenize raw text for TLE input (without special tokens)
        tle_tokens = self.processor.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_length,
            truncation=True,
            padding=False,
        )

        # Create full target sequence for decoder using proper tokenization
        # Use processor tokenizer with proper prefix tokens
        self.processor.tokenizer.set_prefix_tokens(language=lang, task="transcribe")
        target_tokens = self.processor.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_length,
            truncation=True,
            padding=False,
        )

        return {
            "input_ids": tle_tokens["input_ids"].squeeze(),  # Raw text for TLE
            "attention_mask": tle_tokens["attention_mask"].squeeze(),
            "labels": target_tokens["input_ids"].squeeze(),  # Full sequence for decoder
            "text": text,
            "language": lang,
        }


class TextOnlyCollator:
    """Collator for text-only fine-tuning batches."""

    def __init__(self, processor, max_length=256, language_mapping=None):
        self.processor = processor
        self.max_length = max_length
        self.language_mapping = language_mapping or {"en": 0, "zh": 1, "yue": 2}

    def __call__(self, batch):
        input_ids = []
        attention_masks = []
        labels = []
        lang_ids = []

        for item in batch:
            input_ids.append(item["input_ids"])
            attention_masks.append(item["attention_mask"])
            labels.append(item["labels"])
            lang_ids.append(self.language_mapping.get(item["language"], 0))

        # Pad sequences
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids,
            batch_first=True,
            padding_value=self.processor.tokenizer.pad_token_id,
        )
        attention_masks = torch.nn.utils.rnn.pad_sequence(
            attention_masks, batch_first=True, padding_value=0
        )
        labels = torch.nn.utils.rnn.pad_sequence(
            labels, batch_first=True, padding_value=-100  # Ignore in loss
        )

        return {
            "input_ids": input_ids,
            "attention_mask": attention_masks,
            "labels": labels,
            "lang_ids": torch.tensor(lang_ids, dtype=torch.long),
        }


def main():
    parser = argparse.ArgumentParser(
        description="Fine-tune Whisper decoder with TLE pseudo states"
    )
    parser.add_argument(
        "--tle_checkpoint",
        type=str,
        required=True,
        help="Path to trained TLE model checkpoint",
    )
    parser.add_argument(
        "--dataset", type=str, required=True, help="HuggingFace dataset name or path"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="checkpoints/whisper-finetuned",
        help="Output directory for checkpoints",
    )
    parser.add_argument("--batch_size", type=int, default=4, help="Training batch size")
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-5,
        help="Learning rate for decoder fine-tuning",
    )
    parser.add_argument(
        "--max_epochs", type=int, default=3, help="Maximum training epochs"
    )
    parser.add_argument(
        "--max_steps", type=int, default=None, help="Maximum training steps"
    )
    parser.add_argument(
        "--save_every", type=int, default=1000, help="Save checkpoint every N steps"
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Device to use (auto-detect if not specified)",
    )
    parser.add_argument(
        "--use_wandb", action="store_true", help="Enable Weights & Biases logging"
    )
    parser.add_argument(
        "--train_split", type=str, default="train", help="Training split name"
    )
    parser.add_argument(
        "--val_split", type=str, default="validation", help="Validation split name"
    )

    args = parser.parse_args()

    # Auto-detect device
    device = args.device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load models
    whisper_model, processor = load_whisper_model(device)
    tle_model = load_tle_model(args.tle_checkpoint, device)

    # Load dataset
    print(f"Loading dataset: {args.dataset}")
    try:
        dataset = load_dataset(args.dataset)
    except Exception as e:
        print(f"Failed to load dataset: {e}")
        return

    # Create datasets
    train_dataset = TextOnlyDataset(dataset, processor, split=args.train_split)
    val_dataset = TextOnlyDataset(dataset, processor, split=args.val_split)

    # Create collator
    collator = TextOnlyCollator(processor)

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collator,
        num_workers=0,
        pin_memory=True,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collator,
        num_workers=0,
        pin_memory=True,
    )

    # Create model
    model = WhisperDecoderFinetune(
        tle_model, whisper_model, processor, learning_rate=args.learning_rate
    )

    # Callbacks
    os.makedirs(args.output_dir, exist_ok=True)
    checkpoint_callback = ModelCheckpoint(
        dirpath=args.output_dir,
        filename="whisper-decoder-{epoch:02d}-{step:06d}",
        save_top_k=-1,
        every_n_train_steps=args.save_every,
    )

    # Trainer
    trainer_kwargs = {
        "max_epochs": args.max_epochs,
        "callbacks": [checkpoint_callback],
        "accelerator": device if device != "cpu" else "cpu",
        "devices": 1,
        "log_every_n_steps": 10,
    }
    if args.use_wandb:
        trainer_kwargs["logger"] = WandbLogger(project="whisper-decoder-finetuning")
    if args.max_steps:
        trainer_kwargs["max_steps"] = args.max_steps

    # Use bfloat16 if available
    if device == "cuda" and torch.cuda.is_bf16_supported():
        trainer_kwargs["precision"] = "bf16-mixed"

    trainer = Trainer(**trainer_kwargs)

    # Train
    print("Starting Whisper decoder fine-tuning...")
    trainer.fit(model, train_loader, val_loader)

    print(f"Training complete! Checkpoints saved to {args.output_dir}")


if __name__ == "__main__":
    main()
