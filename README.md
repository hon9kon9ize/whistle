# üó£Ô∏è Whisper-TLE: Text-to-Latent VAE for Text-Only Fine-Tuning of Whisper

> **Text-to-Latent Encoder (TLE)** ‚Äî a multilingual VAE that learns to map text tokens to pseudo audio encoder states for OpenAI‚Äôs `whisper-large-v3`.  
> Enables text-only adaptation and domain tuning *without* paired speech data.

---

## üåü Overview

Unoffical implementation of **Text-to-Latent VAE (TLE)** described in  
üìÑ *‚ÄúWhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers‚Äù* ([arXiv:2509.10452](https://arxiv.org/abs/2509.10452)).

The goal is to fine-tune Whisper (or any seq2seq ASR model) on *text-only* data by replacing the frozen speech encoder with a **variational text encoder** that generates *latent speech representations* compatible with Whisper‚Äôs decoder.

---

## üß© Key Features

- üîÑ **Drop-in Whisper-compatible encoder replacement**  
  Produces `T √ó H` hidden states (H=1280 for whisper-large-v3) that match the speech encoder output.

- üß† **VAE formulation**  
  Global latent `z ~ N(Œº, œÉ¬≤)` with residual Conv1D FiLM modulation and Œ≤-VAE training objective.

- üåç **Multilingual support**  
  Optional language embeddings and temperature-balanced sampling for multilingual fine-tuning.

- üó£Ô∏è **Two-phase workflow**
  1. **Supervised TLE training**: regress text ‚Üí teacher encoder states from paired speech‚Äìtext.  
  2. **Text-only fine-tuning**: replace encoder with TLE, continue training Whisper decoder.

---

## üèóÔ∏è Architecture Summary

```

Text tokens ‚îÄ‚ñ∫ Transformer Encoder ‚îÄ‚ñ∫ Œº, logœÉ¬≤ ‚îÄ‚ñ∫ sample z
‚îÇ
‚ñº
Linear (text‚ÜíH) ‚Üí interpolate to T ‚Üí + PosEnc
‚îÇ
‚ñº
Residual Conv1D + FiLM(z)
‚îÇ
‚ñº
Whisper-like states (B, T, H)

````

Loss:
\[
\mathcal{L} = \| E_{\text{teacher}} - \tilde{E} \|_2^2 + \beta\, \mathrm{KL}\big(q_\phi(z|y)\,\|\,\mathcal{N}(0,I)\big)
\]

---

## üß∞ Installation

```bash
git clone https://github.com/hon9kon9ize/whisper-tle.git
cd whisper-tle

# Python 3.10+ recommended
pip install -r requirements.txt
````

`requirements.txt` should include:

```text
torch>=2.1
transformers>=4.45
numpy
tqdm
soundfile
```

---

## üöÄ Usage

### 1Ô∏è‚É£ Train TLE on paired data

```python
from transformers import WhisperProcessor
from tle.modeling_tle import TLEVAE, TLEVAEConfig, vae_loss
from tle.utils import get_teacher_states

# Load Whisper encoder as teacher
processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3")
teacher = WhisperModel.from_pretrained("openai/whisper-large-v3").eval().cuda()

# Init TLE
cfg = TLEVAEConfig(vocab_size=processor.tokenizer.vocab_size,
                   whisper_hidden=teacher.config.d_model)
tle = TLEVAE(cfg).cuda()

# Forward pass
E_teacher = get_teacher_states(teacher, audio_list)  # (B, T, 1280)
E_tilde, mu, logvar = tle(input_ids, attention_mask, target_T=E_teacher.size(1))

loss = vae_loss(E_tilde, E_teacher, mu, logvar, beta=cfg.beta)
loss.backward()
```

### 2Ô∏è‚É£ Fine-tune Whisper decoder with text-only data

```python
from transformers import WhisperForConditionalGeneration, BaseModelOutput

asr = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3").cuda()
asr.model.encoder.requires_grad_(False)  # freeze speech encoder

# Use TLE to generate pseudo encoder states
E_tilde, _, _ = tle(input_ids, attention_mask)
encoder_outputs = BaseModelOutput(last_hidden_state=E_tilde)

loss = asr(encoder_outputs=encoder_outputs, labels=labels).loss
loss.backward()
```

---

## üåç Multilingual Training

To build a multilingual TLE:

* Add a `lang_embed = nn.Embedding(num_langs, text_hidden)` and sum it into the token embeddings.
* Use **temperature-based sampling** to balance language proportions:
  [
  p_l \propto n_l^\alpha, \ \alpha < 1
  ]
* Include typologically diverse languages (different families & scripts).
* Optionally train with TTS-generated audio for low-resource languages.

---

## üìä Recommended Training Schedule

| Stage | Data              | Objective     | Steps | Notes                                 |
| ----- | ----------------- | ------------- | ----- | ------------------------------------- |
| 1     | Paired audio‚Äìtext | `MSE + Œ≤¬∑KL`  | 100k  | Freeze Whisper; train TLE             |
| 2     | Text-only         | `Decoder NLL` | 50k   | Alternate text-only & audio steps     |
| 3     | Optional          | Joint TLE+TTS | 30k   | Combine both latent & TTS supervision |

---

## üß™ Evaluation

After training, you can evaluate WER or CER on out-of-domain test sets by:

1. Using the frozen Whisper encoder (for audio evaluation), or
2. Using TLE-generated encoder states from text (for domain adaptation).

The paper reports that **TLE+TTS** outperforms either approach alone across multiple languages.

---

## üìÅ Repository Structure

```
whistle/
‚îú‚îÄ‚îÄ tle/
‚îÇ   ‚îú‚îÄ‚îÄ tle.py        # TLEVAE + ResidualConv1dFiLM + loss
‚îú‚îÄ‚îÄ teacher.py        # feature extraction, teacher state cache
‚îú‚îÄ‚îÄ train.py          # supervised training loop
‚îî‚îÄ‚îÄ README.md
```

**Planned additions:**
- `tle/utils.py` - data loading utilities and helper functions
- `tle/data.py` - dataset classes and multilingual sampling  
- `finetune_decoder.py` - text-only Whisper decoder fine-tuning
- `requirements.txt` - project dependencies

---

## üìú Citation

```bibtex
@misc{pandey2025whistledeeplysupervisedtextonly,
      title={WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers}, 
      author={Akshat Pandey and Karun Kumar and Raphael Tang},
      year={2025},
      eprint={2509.10452},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.10452}, 
}
```